{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0290ad37-f136-49f3-ba28-7b505f3499cb",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs (DCGANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45bb42-42c2-4f57-af69-b5f62d90226c",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "In a neural network, each $i$'th node $a_i^{(l)}$ of a layer $l$ calculates as\n",
    "\n",
    "$$\n",
    "a_i^{(l)} = g^{(l)}(z_i^{(l)})\n",
    "$$\n",
    "\n",
    "with the **activation function** $g^{(l)}$ and\n",
    "\n",
    "$$\n",
    "z_i^{(l)} = \\sum_i W_i^{(l)} a_i^{(l-1)}\n",
    "$$\n",
    "\n",
    "Activation functions are \n",
    "\n",
    "1. **Non-linear** to approximate complex functions\n",
    "2. **Differentiable** for backpropagation\n",
    "\n",
    "## Common Activation Functions\n",
    "\n",
    "### ReLU\n",
    "\n",
    "ReLU (Rectified Linear Unit)\n",
    "\n",
    "$$\n",
    "g(z) = max(0, z)\n",
    "$$\n",
    "\n",
    "has a **Dying ReLU problem**.\n",
    "\n",
    "### Leaky ReLU\n",
    "\n",
    "Leaky ReLU solves the dying ReLU problem:\n",
    "\n",
    "$$\n",
    "g(z) = max(\\alpha z, z)\n",
    "$$\n",
    "\n",
    "The value $0 \\le \\alpha \\le 1$ is typically small\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "Sigmoid has values between **0** and **1**:\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Sigmoid has a vanishing gradient and saturation problem.\n",
    "\n",
    "### Tanh\n",
    "\n",
    "Tanh has values between **-1** and **1**, hence keeps the sign of the input:\n",
    "\n",
    "$$\n",
    "g(z) = tanh(z)\n",
    "$$\n",
    "\n",
    "Tanh has also a vanishing gradient and saturation problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd658a9-9a37-4e9f-8746-2f101e0ebb41",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "1. Batch normalization smooths the cost function\n",
    "2. Batch normalization **reduces** the internal **covariate shift**\n",
    "3. Batch normalization **speeds up learning**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee46ea5-29e2-4d0d-a17e-2711942679b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
